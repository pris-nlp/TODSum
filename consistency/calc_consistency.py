import json
import argparse
import collections
from numpy import *
import pandas as pd

DOMAIN_MAP = {
    "restaurant": ["restaurant", "seat", "seats", "food"],
    "hotel": ["hotel", "house", "room", "stay", "hotel's"],
    "attraction": ["attraction", "attaction", "entrance", "college", "multiple sports", "architecture"],
    "train": ["train", "ticket"],
    "taxi": ["taxi", "car "]
}

INTENT_MAP = {
    "book": ["book", "reserve", "buy", "perchase", "accommodate", "purchase", "tickets"],
    "request": ["postcode", "phone", "address", "check", "search", "seach", "trainid", "duration", "the user wants to know", "user wants to know", "the user asks for", "the user wonders", "user wonders what"]
}

SLOT_MAP = {
    'hotel': {
        'people': ['people', 'person'],
        'stay': ['day', 'days'],
        'stars': ['star'],
        'yes': ['has', 'with', 'parking']

    },
    'taxi':{
        'leaveAt': ['leaves at', 'leaves', 'will leave', 'will leave at'],
        'arriveBy': ['arrive at', 'arrives', 'will arrive', 'will arrive at']
    }
}

def read_files(dst_path, summary_path, ontology_file):
    with open(dst_path, 'r') as fdst, open(summary_path, 'r') as fsum, open(ontology_file, 'r') as fonto:
        dst_datas = fdst.readlines()
        summary_datas = fsum.readlines()
        ontology = json.load(fonto)

        summary_datas = summary_datas[:999]

        assert len(dst_datas) == len(summary_datas)

    return dst_datas, summary_datas, ontology


def process_dst(dst_datas):
    '''
    Process each dialogue states, and count the number of slots in gold dialogue states
    dst_datas: file of gold dst. eg: test.oracle
    '''
    processed_dsts = []
    value_nums = []
    all_dst_values = []
    for i, dst_data in enumerate(dst_datas):
        dsts = dst_data.strip().split(")")
        dsts = [x for x in dsts if x != ""]

        tmp_map = {}
        value_num = 0
        dst_values = []
        for line in dsts:
            tmp = {}
            splited_line = line.strip().split(" ( ")
            if len(splited_line) == 1:
                continue
                
            domain = splited_line[0].split(" ")[0]
            domain = domain.strip()
            intent = splited_line[0].split(" ")[1]
            intent = intent.strip()
            pairs = splited_line[1].split(" ; ")
            pairs = [x.split(" = ") for x in pairs]
            pairs = [x for x in pairs if len(x) == 2]

            value_num += len(pairs)
            tmp[intent] = pairs
            if domain in tmp_map.keys():
                tmp_map[domain] = dict(list(tmp_map[domain].items()) + list(tmp.items()))
            else:
                tmp_map[domain] = tmp
            # print(pairs)
            for k, v in pairs:
                dst_values.append(v)

        processed_dsts.append(tmp_map)
        value_nums.append(value_num)
        all_dst_values.append(dst_values)
    return processed_dsts, value_nums, all_dst_values

def tokenize(summary):
    '''Split the summary utterances'''
    summary = summary.strip().split(".")
    summary = [line for line in summary if line != ""]
    for i in range(len(summary)):
        summary[i] = summary[i].replace("-", " ")
    return summary


def process_ontology(ontology):
    '''Process the ontology set'''
    all_values = []
    for k in ontology.keys():
        all_values += ontology[k]
    return set(all_values)


def slot_filter(values):
    '''De duplicate the summary slot value set obtained for each sample'''
    for i, word in enumerate(values):
        for another in values[:i] + values[i+1:]:
            if word in another:
                del values[i]
                return True
                
    return False
    

def process_summary(summary_datas, ontology):
    '''
    Process the summary generated by the model, and count the number of slot values in the summary
    summary_datas: summary samples from summary file, eg: test.hpy
    ontology: ontology datas from ontology.json
    '''
    ontology = process_ontology(ontology)
    number_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
    banned_word_set = ['the', 'ask', 'restaurant', 'hotel', 'attraction', 'train', 'taxi']
    processed_summarys = []
    summary_value_nums = []
    all_summary_values = []
    for summary in summary_datas:
        summary = tokenize(summary)
        processed_summarys.append(summary)

        tmp_summary_str = ""
        for line in summary:
            line = line.replace("-", " ")
            tmp_summary_str += line

        value_per_sample = []
        for ele in list(ontology):
            if ele in tmp_summary_str:
                if (ele not in number_set and len(ele) <= 2) or ele in banned_word_set:
                    continue
                else:
                    value_per_sample.append(ele)
        
        while slot_filter(value_per_sample):
            pass

        summary_value_nums.append(len(value_per_sample))
        all_summary_values.append(value_per_sample)
    
    return processed_summarys, summary_value_nums, all_summary_values


def which_domain_intent(summary_sentence):
    domain = ""
    for k in DOMAIN_MAP.keys():
        for line in DOMAIN_MAP[k]:
            if line in summary_sentence:
                domain = k
                break
    
    intent = ""
    for kk in INTENT_MAP.keys():
        for line in INTENT_MAP[kk]:
            if line in summary_sentence:
                intent = kk
                break
    if not intent:
        intent = "inform"
    
    return domain, intent


def value_type(sentence, value, domain):
    sentence = sentence.split(" ")
    sentence = [s.strip(",") for s in sentence if s != ""]
    number_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
    # print(sentence)
    if value not in sentence:
        return
    v_idx = sentence.index(value)
    if domain == "hotel" and value in number_set:
        for k in SLOT_MAP[domain].keys():
            for line in SLOT_MAP[domain][k]:
                if line in sentence[v_idx : v_idx + 2]:
                    return k
    else:
        for k in SLOT_MAP[domain].keys():
            for line in SLOT_MAP[domain][k]:
                if line in sentence[v_idx - 2 : v_idx + 2]:
                    return k



def calc_matchnum(processed_summarys, processed_dsts, all_summary_values, all_dst_values):
    '''Count the number of dialog states that overlap the golden dialog state in the generated summary'''

    assert len(processed_dsts) == len(processed_summarys) == len(all_summary_values) == len(all_dst_values)
    number_set = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
    correct_nums = []

    for i, summary in enumerate(processed_summarys):
        correct_num_per_sample = 0
        for sentence in summary:
            correct_num_per_sentence = 0
            # domain and intent
            domain, intent = which_domain_intent(sentence)
            if intent == "request":
                continue
            # slots and values
            values = []
            for line in all_summary_values[i]:
                if line in sentence:
                    values.append(line)
            
            words = sentence.split(" ")
            words = [w for w in words if w != ""]
            counter = collections.defaultdict(int)
            for w in words:
                if w in number_set:
                    counter[w] += 1
            for k in counter.keys():
                while values.count(k) < counter[k]:
                    values.append(k)

            if domain not in processed_dsts[i].keys():
                continue
            if intent not in processed_dsts[i][domain].keys():
                continue
            corr_dst = processed_dsts[i][domain][intent]
            corr_values = [[x[1]] for x in corr_dst]
            
            pred_pairs = []
            for v in values:
                if domain == "hotel":
                    if v in number_set:
                        v_slot = value_type(sentence, v, domain)
                        if not v_slot:
                            pred_pairs.append([v])
                        else:
                            pred_pairs.append([v_slot, v])
                    elif v in ['park', 'praking', 'internet']:
                        if v == "park":
                            v = "parking"
                        v_slot = value_type(sentence, v, domain)
                        if not v_slot:
                            pred_pairs.append([v])
                        else:
                            pred_pairs.append([v, v_slot])
                    else:
                        pred_pairs.append([v])
                elif domain == "taxi":
                    if ":" in v:
                        v_slot = value_type(sentence, v, domain)
                        if not v_slot:
                            pred_pairs.append([v])
                        else:
                            pred_pairs.append([v_slot, v])
                    else:
                        pred_pairs.append([v])
                else:
                    pred_pairs.append([v])

            # compare
            for item in pred_pairs:
                if len(item) == 2:
                    if item in corr_dst:
                        correct_num_per_sentence += 1
                elif len(item) == 1:
                    if item in corr_values:
                        correct_num_per_sentence += 1

            correct_num_per_sample += correct_num_per_sentence

        correct_nums.append(correct_num_per_sample)
    
    return correct_nums


def calc_scores(correct_nums, dst_value_nums, summary_value_nums):
    assert len(correct_nums) == len(dst_value_nums) == len(summary_value_nums)
    precision_list = [correct_nums[i] / summary_value_nums[i] for i in range(len(correct_nums)) if summary_value_nums[i] != 0]
    precision = mean(precision_list)
    recall_list = [correct_nums[i] / dst_value_nums[i] for i in range(len(correct_nums)) if dst_value_nums[i] != 0]
    recall = mean(recall_list)
    overall_f1 = (2*precision*recall) / (precision + recall)
    overall_f1_list = []
    for i in range(len(recall_list)):
        if recall_list[i] + precision_list[i] != 0:
            overall_f1_list.append((2*precision_list[i]*recall_list[i]) / (precision_list[i] + recall_list[i]))
        else:
            overall_f1_list.append(0)
    # overall_f1_list = [(2*precision_list[i]*recall_list[i]) / (precision_list[i] + recall_list[i]) for i in range(len(recall_list))]

    return precision, recall, overall_f1, overall_f1_list


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dst_path", type = str, default="./test.oracle")
    parser.add_argument("--summary_path", type = str, default="./decode_result/pgn_predict.hpy")
    args = parser.parse_args()
    ontology_file = "./ontology.json"

    dst_datas, summary_datas, ontology = read_files(args.dst_path, args.summary_path, ontology_file)

    processed_dsts, dst_value_nums, all_dst_values = process_dst(dst_datas)
    processed_summarys, summary_value_nums, all_summary_values = process_summary(summary_datas, ontology)
    
    correct_nums = calc_matchnum(processed_summarys, processed_dsts, all_summary_values, all_dst_values)
    precision, recall, overall_f1, overall_f1_list = calc_scores(correct_nums, dst_value_nums, summary_value_nums)

    print("precision: ", precision)
    print("recall: ", recall)
    print("overall_f1: ", overall_f1)


if __name__ == "__main__":
    main()

